{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "49538487-3502-4153-a997-f3b1f1b26334"
    }
   },
   "source": [
    "# Deep Learning for NLP with Keras\n",
    "\n",
    "\n",
    "This notebook is meant to give you the tools to get started with using neural networks.\n",
    "We'll be using the toolkit Keras, which is a very up-to-date and sufficiently high-level Python package for quick development of Deep Learning systems.\n",
    "\n",
    "In this workshop, we'll be focussing on approaches relevant to NLP.\n",
    "We'll cover both a simple classification setting, as well as a sequence prediction setting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "767d549c-69f8-4c94-8e9e-a5229de9cf8f"
    }
   },
   "source": [
    "<img src=\"files/flowchart.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/neural_network_1.tiff\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/neural_network_multiclass.tiff\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/multiclass_onehot.tiff\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "3012f0d5-b137-4e58-9196-74fd557ebabc"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# Begin by trying to import the necessary packages (click this cell, and then Shift+Enter)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # Freeze seeds for reproducibility\n",
    "import keras\n",
    "#from keras.utils.visualize_util import plot as keras_plot\n",
    "from IPython.display import SVG\n",
    "from keras.utils.visualize_util import model_to_dot\n",
    "\n",
    "from codecs import open\n",
    "from collections import defaultdict\n",
    "\n",
    "# Prepare word/tag to ID conversions\n",
    "fillers = ('<w>', '</w>', '<PAD>', '_UNK')\n",
    "word_to_id = defaultdict(lambda: len(word_to_id)+1) # 0 for padding\n",
    "tag_to_id = defaultdict(lambda: len(tag_to_id)+1)\n",
    "for word in fillers:\n",
    "    word_to_id[word]\n",
    "    if word != '_UNK':\n",
    "        tag_to_id[word]\n",
    "    \n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "db1b56c4-be4c-4029-818b-dedf89609106"
    }
   },
   "source": [
    "If that worked, you're good to go!\n",
    "\n",
    "Next, load these predefined functions for reading and preparing data for our neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "83b24b76-a41d-4b07-98ee-f2518923a49a"
    }
   },
   "outputs": [],
   "source": [
    "def read_ffnn_data(fname, delimiter='\\t'):\n",
    "    '''Read data from a tsv file with format: <word<TAB>tag<NEWLINE>'''\n",
    "    X, y = [], []\n",
    "    with open(fname, 'r', encoding='utf-8') as in_f:\n",
    "        for line in in_f:\n",
    "            if len(line) <= 2: continue\n",
    "            word, tag = line.strip().split(delimiter)\n",
    "            X.append([word_to_id.get(word, word_to_id['_UNK'])])\n",
    "            y.append(tag_to_id[tag])\n",
    "            \n",
    "    return X, y\n",
    "\n",
    "def read_rnn_data(fname, delimiter='\\t'):\n",
    "    '''\n",
    "    Read data from a tsv file with format: <word<TAB>tag<NEWLINE>,\n",
    "    and blank lines between each sentence.\n",
    "    '''\n",
    "    X, y = [], []\n",
    "    with open(fname, 'r', encoding='utf-8') as in_f:\n",
    "        curr_X, curr_y = [word_to_id['<w>']], [tag_to_id['<w>']]\n",
    "        for line in in_f:\n",
    "            if len(line) <= 2: # i.e. newline\n",
    "                if curr_X and curr_y:\n",
    "                    if len(curr_X) <= 20:\n",
    "                        curr_X.append(word_to_id['</w>'])\n",
    "                        curr_y.append(tag_to_id['</w>'])\n",
    "                        X.append(curr_X)\n",
    "                        y.append(curr_y)\n",
    "                    curr_X = [word_to_id['<w>']]\n",
    "                    curr_y = [tag_to_id['<w>']]\n",
    "                continue\n",
    "            word, tag = line.strip().split(delimiter)\n",
    "            curr_X.append(word_to_id.get(word, word_to_id['_UNK']))\n",
    "            curr_y.append(tag_to_id[tag])\n",
    "            \n",
    "    # Catch last line\n",
    "    if X and y:\n",
    "        curr_X.append(word_to_id['</w>'])\n",
    "        curr_y.append(tag_to_id['</w>'])\n",
    "        X.append(curr_X)\n",
    "        y.append(curr_y)\n",
    "            \n",
    "    return X, y\n",
    "\n",
    "def read_word_embeddings(fname):\n",
    "    '''Read an embeddings file matching the input vocab'''\n",
    "    word_vec_map = {}\n",
    "    with open(fname, 'r', encoding='utf-8') as in_f:\n",
    "        for line in in_f:\n",
    "            fields = line.strip().split()\n",
    "            word = fields[0]\n",
    "            embedding = np.asarray([float(i) for i in fields[1:]], dtype=np.float32)\n",
    "            word_to_id[word]\n",
    "            word_vec_map[word] = embedding\n",
    "    \n",
    "    return word_vec_map, len(embedding)\n",
    "\n",
    "def y_to_onehot(y, n_classes):\n",
    "    y_onehot = np.zeros((len(y), n_classes), dtype=np.int32)\n",
    "    for idx, tag in enumerate(y):\n",
    "        y_onehot[idx, tag] = 1\n",
    "        \n",
    "    return y_onehot\n",
    "\n",
    "def y_to_onehot_rnn(y):\n",
    "    y_onehot = np.zeros((len(y), max_sent_len, n_classes), dtype=np.int32)\n",
    "    for idx, sentence in enumerate(y):\n",
    "        for idy, tag in enumerate(sentence):\n",
    "            if idy >= max_sent_len: break\n",
    "            y_onehot[idx, idy+(max_sent_len-len(sentence)), tag] = 1\n",
    "            \n",
    "   # sequence.pad_sequences(y_onehot, maxlen=max_sent_len, dtype=np.int32, value=tag_to_id['<PAD>'])\n",
    "            \n",
    "    return y_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "bed97432-8b2f-4897-b3c4-6248a74a0084"
    }
   },
   "source": [
    "## Feed-Forward Neural Networks\n",
    "\n",
    "So that our neural network can make sense of our data, we'll be working with numerical IDs from here on.\n",
    "\n",
    "We'll also load some pre-trained word representations. Such 'word embeddings' are a big part of the success of neural NLP applications. Using very simple techniques, distributional word representatiosn can be learnt both quickly and without any supervision at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "d9f6d0b7-36e0-465f-82e5-ee807081db66"
    }
   },
   "outputs": [],
   "source": [
    "word_to_vec, dimensionality = read_word_embeddings('sv.polyglot.filtered')\n",
    "vocab_size = len(word_to_vec)+1\n",
    "embedding_weights = np.zeros((vocab_size, dimensionality), dtype=np.float32)\n",
    "for word, index in word_to_id.items():\n",
    "    embedding_weights[index,:] = word_to_vec[word]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "9074300e-5c8b-4f92-b8d6-71f0e6148882"
    }
   },
   "source": [
    "Let's have a look at how some word representations are similar/different to each other (*low = similar, high = dissimilar*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "da295db5-0226-4301-ad25-26d678357845"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos(gul, vit): 0.199402826075\n",
      "cos(usa, ger): 0.130351893484\n",
      "cos(vit, ger): 0.832578361366\n",
      "cos(usa, gul): 0.908857627253\n"
     ]
    }
   ],
   "source": [
    "gul = word_to_vec['gul']\n",
    "vit = word_to_vec['vit']\n",
    "usa = word_to_vec['USA']\n",
    "ger = word_to_vec['Tyskland']\n",
    "import scipy\n",
    "print('cos(gul, vit): {0}'.format(scipy.spatial.distance.cosine(gul, vit)))\n",
    "print('cos(usa, ger): {0}'.format(scipy.spatial.distance.cosine(usa, ger)))\n",
    "print('cos(vit, ger): {0}'.format(scipy.spatial.distance.cosine(vit, ger)))\n",
    "print('cos(usa, gul): {0}'.format(scipy.spatial.distance.cosine(usa, gul)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d0065840-742e-4c75-be5f-eb17cc2a0c39"
    }
   },
   "source": [
    "Next, we'll load some Part-of-Speech (POS) data.\n",
    "We'll use a subset of the Swedish Universal Dependencies POS data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "9dedda3e-2534-4f5c-beb2-bb852f6ccc69"
    }
   },
   "outputs": [],
   "source": [
    "train_X, train_y = read_ffnn_data('sv-ud-train-small.conllu')\n",
    "dev_X, dev_y = read_ffnn_data('sv-ud-dev-small.conllu')\n",
    "test_X, test_y = read_ffnn_data('sv-ud-test-small.conllu')\n",
    "\n",
    "vocab_size = len(word_to_vec)+1\n",
    "n_classes = len(tag_to_id)+1\n",
    "\n",
    "train_y_onehot = y_to_onehot(train_y, n_classes)\n",
    "dev_y_onehot = y_to_onehot(dev_y, n_classes)\n",
    "test_y_onehot = y_to_onehot(test_y, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the data, and calculate a simple baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEXCAYAAABWNASkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nECXk/wNHTUpkVl6zpAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113bcaa90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "class_counts = Counter(train_y)\n",
    "\n",
    "classes = [pos_class for pos_class, idx in sorted(tag_to_id.items())]\n",
    "counts = [class_counts.get(tag_to_id[pos_class], 0) / float(len(train_y)) for pos_class in classes]\n",
    "\n",
    "plt.bar(range(len(counts)), counts, tick_label=classes)\n",
    "locs, labels = plt.xticks()\n",
    "plt.setp(labels, rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More than 25% of the data belongs to the 'NOUN' class. A simple baseline we can use is to assign this majority class to all instances in our test data. A more fair comparison is to compare with the most frequent class per type in the training data, rather than the training data as a whole. This is essentially what a Naïve Bayes classifier would do, given only unigram features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77934272300469487"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import naive_bayes\n",
    "clf = naive_bayes.MultinomialNB()\n",
    "clf.fit(y_to_onehot(train_X, vocab_size), train_y)\n",
    "clf.score(y_to_onehot(dev_X, vocab_size), dev_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a1a8522f-6500-4d63-9b36-b342e665adf7"
    }
   },
   "source": [
    "Okay, that's a bit more challenging to beat!\n",
    "\n",
    "For our first experiments, we're using a simple type of Neural Networks: The Feed-Forward Neural Network (FFNN).\n",
    "\n",
    "In our case, the network will look something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "8d5b23b6-0c1f-4c62-a95d-d1772c699524"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers.core import Dense, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "def build_ffnn(n_layers, pretrained_embeddings):\n",
    "    '''\n",
    "    Build a simple Feed-Forward Neural Network.\n",
    "    '''\n",
    "    word_input = Input(shape=(1, ), dtype='int32', name='word_input') # One word at a time\n",
    "    if pretrained_embeddings:\n",
    "        word_embedding = Embedding(vocab_size, dimensionality, input_length=1, weights=[embedding_weights])(word_input)\n",
    "    else:\n",
    "        word_embedding = Embedding(vocab_size, dimensionality, input_length=1)(word_input)\n",
    "        \n",
    "    word_embedding = Flatten()(word_embedding)\n",
    "    \n",
    "    # Hidden layer(s)\n",
    "    layer_dim = dimensionality * 2\n",
    "    hidden = Dense(layer_dim)(word_embedding)\n",
    "    for n in range(n_layers-1):\n",
    "        hidden = Dense(layer_dim)(hidden)\n",
    "        layer_dim = int(layer_dim/2.0)\n",
    "    \n",
    "    model_output = Dense(n_classes, activation='softmax', name='main_output')(hidden)\n",
    "    model = Model(input=[word_input], output=[model_output])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "cf42fbb3-4450-4320-b4ec-cf12ff882138"
    }
   },
   "source": [
    "Try running the following block, and make a note of the results.\n",
    "\n",
    "Try changing some parameters (n layers, not using pretrained embeddings, n epochs) and see how things change.\n",
    "**Is going deeper better?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "653d700a-4c2f-4539-a13b-c7d30d297ffc"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "word_input (InputLayer)          (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_14 (Embedding)         (None, 1, 64)         849216      word_input[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)              (None, 64)            0           embedding_14[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 128)           8320        flatten_7[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "main_output (Dense)              (None, 20)            2580        dense_7[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 860116\n",
      "____________________________________________________________________________________________________\n",
      "Train on 18633 samples, validate on 1917 samples\n",
      "Epoch 1/10\n",
      "0s - loss: 2.4645 - acc: 0.3762 - val_loss: 2.0936 - val_acc: 0.4940\n",
      "Epoch 2/10\n",
      "0s - loss: 1.7489 - acc: 0.6058 - val_loss: 1.6161 - val_acc: 0.5952\n",
      "Epoch 3/10\n",
      "0s - loss: 1.3779 - acc: 0.6741 - val_loss: 1.3121 - val_acc: 0.7011\n",
      "Epoch 4/10\n",
      "0s - loss: 1.1421 - acc: 0.7432 - val_loss: 1.1100 - val_acc: 0.7491\n",
      "Epoch 5/10\n",
      "0s - loss: 0.9861 - acc: 0.7866 - val_loss: 0.9710 - val_acc: 0.7846\n",
      "Epoch 6/10\n",
      "0s - loss: 0.8783 - acc: 0.8103 - val_loss: 0.8707 - val_acc: 0.8033\n",
      "Epoch 7/10\n",
      "0s - loss: 0.7999 - acc: 0.8172 - val_loss: 0.7953 - val_acc: 0.8138\n",
      "Epoch 8/10\n",
      "0s - loss: 0.7406 - acc: 0.8205 - val_loss: 0.7366 - val_acc: 0.8190\n",
      "Epoch 9/10\n",
      "0s - loss: 0.6940 - acc: 0.8329 - val_loss: 0.6882 - val_acc: 0.8440\n",
      "Epoch 10/10\n",
      "0s - loss: 0.6565 - acc: 0.8372 - val_loss: 0.6505 - val_acc: 0.8451\n",
      "Evaluation - test_loss: 0.7160 - test_acc: 0.7999\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"337pt\" viewBox=\"0.00 0.00 185.00 337.00\" width=\"185pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 333)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-333 181,-333 181,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 4763877904 -->\n",
       "<g class=\"node\" id=\"node1\"><title>4763877904</title>\n",
       "<polygon fill=\"none\" points=\"10.5,-292.5 10.5,-328.5 166.5,-328.5 166.5,-292.5 10.5,-292.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"88.5\" y=\"-306.8\">word_input (InputLayer)</text>\n",
       "</g>\n",
       "<!-- 4763879120 -->\n",
       "<g class=\"node\" id=\"node2\"><title>4763879120</title>\n",
       "<polygon fill=\"none\" points=\"0,-219.5 0,-255.5 177,-255.5 177,-219.5 0,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"88.5\" y=\"-233.8\">embedding_14 (Embedding)</text>\n",
       "</g>\n",
       "<!-- 4763877904&#45;&gt;4763879120 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>4763877904-&gt;4763879120</title>\n",
       "<path d=\"M88.5,-292.313C88.5,-284.289 88.5,-274.547 88.5,-265.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"92.0001,-265.529 88.5,-255.529 85.0001,-265.529 92.0001,-265.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4815344976 -->\n",
       "<g class=\"node\" id=\"node3\"><title>4815344976</title>\n",
       "<polygon fill=\"none\" points=\"30,-146.5 30,-182.5 147,-182.5 147,-146.5 30,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"88.5\" y=\"-160.8\">flatten_7 (Flatten)</text>\n",
       "</g>\n",
       "<!-- 4763879120&#45;&gt;4815344976 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>4763879120-&gt;4815344976</title>\n",
       "<path d=\"M88.5,-219.313C88.5,-211.289 88.5,-201.547 88.5,-192.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"92.0001,-192.529 88.5,-182.529 85.0001,-192.529 92.0001,-192.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4642278736 -->\n",
       "<g class=\"node\" id=\"node4\"><title>4642278736</title>\n",
       "<polygon fill=\"none\" points=\"33.5,-73.5 33.5,-109.5 143.5,-109.5 143.5,-73.5 33.5,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"88.5\" y=\"-87.8\">dense_7 (Dense)</text>\n",
       "</g>\n",
       "<!-- 4815344976&#45;&gt;4642278736 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>4815344976-&gt;4642278736</title>\n",
       "<path d=\"M88.5,-146.313C88.5,-138.289 88.5,-128.547 88.5,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"92.0001,-119.529 88.5,-109.529 85.0001,-119.529 92.0001,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4642277776 -->\n",
       "<g class=\"node\" id=\"node5\"><title>4642277776</title>\n",
       "<polygon fill=\"none\" points=\"21,-0.5 21,-36.5 156,-36.5 156,-0.5 21,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"88.5\" y=\"-14.8\">main_output (Dense)</text>\n",
       "</g>\n",
       "<!-- 4642278736&#45;&gt;4642277776 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>4642278736-&gt;4642277776</title>\n",
       "<path d=\"M88.5,-73.3129C88.5,-65.2895 88.5,-55.5475 88.5,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"92.0001,-46.5288 88.5,-36.5288 85.0001,-46.5289 92.0001,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 10                # Number of epochs\n",
    "n_layers = 1                 # Number of hidden layers\n",
    "pretrained_embeddings = True # Using pretrained embeddings (True/False)\n",
    "\n",
    "model = build_ffnn(n_layers, pretrained_embeddings)\n",
    "model.compile(optimizer='sgd',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "model.fit(train_X, train_y_onehot, validation_data=(dev_X, dev_y_onehot), nb_epoch=n_epochs, batch_size=100, verbose=2);\n",
    "print('Evaluation - test_loss: {0:.4f} - test_acc: {1:.4f}'.format(*model.evaluate(test_X, test_y_onehot, batch_size=100, verbose=2)))\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b4a0dc32-b95e-4558-a39c-4e1b9fffe987"
    }
   },
   "source": [
    "Using pre-trained embeddings, you should have gotten a test accuracy of above 80%.\n",
    "Not too bad, considering the fact that we're not using any context!\n",
    "\n",
    "Now, let's do a small error analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "e8a83ab0-c3f3-47e6-b303-b1ea55600740"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNT: [u'Det', u'k\\xe4nns', u'riktigt', u'att', u'ha', u'ett', u'riktigt', u'yrke', u'.']\n",
      "POS: [u'PRON', u'VERB', u'ADV', u'PART', u'VERB', u'DET', u'ADV', u'NOUN', u'PUNCT']\n",
      "UNK: [False, False, False, False, False, False, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "def ffnn_predict_sentence(sent, model):\n",
    "    word_ids = [word_to_id.get(word, word_to_id['_UNK']) for word in sent]\n",
    "    preds = model.predict_on_batch(word_ids)\n",
    "    pred_pos = [tag for pred in preds for tag, tag_id in tag_to_id.items() if tag_id == np.argmax(pred)]\n",
    "    print('SNT: {0}'.format(sent))\n",
    "    print('POS: {0}\\nUNK: {1}'.format(pred_pos, [idx==word_to_id['_UNK'] for idx in word_ids]))\n",
    "    \n",
    "\n",
    "sent = u'Det känns riktigt att ha ett riktigt yrke .'.split()\n",
    "ffnn_predict_sentence(sent, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d2bcd0f0-6a46-467e-9ba2-9238e6b765d6"
    }
   },
   "source": [
    "Since no context is used, the two instances of 'riktigt' incorrectly receive the same class (should be ADV, ADJ)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "1542954c-3b0d-4a9e-b51a-0797fb9ad948"
    }
   },
   "source": [
    "## Recurrent Neural Networks\n",
    "\n",
    "Using an FFNN is often sufficient, but incorporating context information (of varying lengths) is problematic.\n",
    "\n",
    "A solution to this, is to use Recurrent Neural Networks.\n",
    "(TODO: More info)\n",
    "\n",
    "First, we'll need to read in the data and convert it to a different format.\n",
    "Whereas we used a simple 'one word, one label' format for the FFNN, we now need a format corresponding to a sentence with tags.\n",
    "\n",
    "We also need to specify a maximum sentence length to consider - to keep things running on CPUs in a reasonable time, we'll restrict ourselves to sentences of 20 words or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "4b72058c-b3c5-4c30-97f1-b5912e98a637"
    }
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "def actual_accuracy(act, pred):\n",
    "    '''\n",
    "    Calculate accuracy each batch.\n",
    "    Keras' standard calculation factors in our padding classes. We don't.\n",
    "    '''\n",
    "    act_argm  = K.argmax(act, axis=-1)   # Indices of act. classes\n",
    "    pred_argm = K.argmax(pred, axis=-1)  # Indices of pred. classes\n",
    "\n",
    "    incorrect = K.cast(K.not_equal(act_argm, pred_argm), dtype='float32')\n",
    "    correct   = K.cast(K.equal(act_argm, pred_argm), dtype='float32')\n",
    "    padding   = K.cast(K.equal(K.sum(act), 0), dtype='float32')\n",
    "    start     = K.cast(K.equal(act_argm, 0), dtype='float32')\n",
    "    end       = K.cast(K.equal(act_argm, 1), dtype='float32')\n",
    "\n",
    "    pad_start     = K.maximum(padding, start)\n",
    "    pad_start_end = K.maximum(pad_start, end) # 1 where pad, start or end\n",
    "\n",
    "    # Subtract pad_start_end from correct, then check equality to 1\n",
    "    # E.g.: act: [pad, pad, pad, <s>, tag, tag, tag, </s>]\n",
    "    #      pred: [pad, tag, pad, <s>, tag, tag, err, </s>]\n",
    "    #   correct: [1,     0,   1,   1,   1,   1,   0,    1]\n",
    "    #     p_s_e: [1,     1,   1,   1,,  0,   0,   0,    1]\n",
    "    #  corr-pse: [0,    -1,   0,   0,   1,   1,   0,    0] # Subtraction\n",
    "    # actu_corr: [0,     0,   0,   0,   1,   1,   0,    0] # Check equality to 1\n",
    "    corr_preds   = K.sum(K.cast(K.equal(correct - pad_start_end, 1), dtype='float32'))\n",
    "    incorr_preds = K.sum(K.cast(K.equal(incorrect - pad_start_end, 1), dtype='float32'))\n",
    "    total = corr_preds + incorr_preds\n",
    "    accuracy = corr_preds / total\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "train_X_rnn, train_y_rnn = read_rnn_data('sv-ud-train.conllu')\n",
    "dev_X_rnn, dev_y_rnn = read_rnn_data('sv-ud-dev.conllu')\n",
    "test_X_rnn, test_y_rnn = read_rnn_data('sv-ud-test-small.conllu')\n",
    "\n",
    "max_sent_len = 20#max(len(sent) for sent in train_X)\n",
    "vocab_size = len(word_to_vec)+1\n",
    "n_classes = len(tag_to_id)+1\n",
    "\n",
    "train_y_rnn = y_to_onehot_rnn(train_y_rnn)\n",
    "dev_y_rnn = y_to_onehot_rnn(dev_y_rnn)\n",
    "test_y_rnn = y_to_onehot_rnn(test_y_rnn)\n",
    "\n",
    "train_X_rnn = sequence.pad_sequences(train_X_rnn, maxlen=max_sent_len, dtype=np.int32, value=word_to_id['<PAD>'])\n",
    "dev_X_rnn = sequence.pad_sequences(dev_X_rnn, maxlen=max_sent_len, dtype=np.int32, value=word_to_id['<PAD>'])\n",
    "test_X_rnn = sequence.pad_sequences(test_X_rnn, maxlen=max_sent_len, dtype=np.int32, value=word_to_id['<PAD>'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "4c1ae737-8b70-47cd-b040-d374af46b9a9"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32),\n",
       " array([    3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3,     3,     3,     3,     3,     1, 10668,  7379,     9,\n",
       "            4,     2], dtype=int32))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y_rnn[0],train_X_rnn[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "c7940def-8980-4383-b844-c677080cc1ac"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, SimpleRNN, merge, BatchNormalization\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "def build_rnn(bidirectional, pretrained_embeddings, rnn_type, fancy, deep):\n",
    "    '''\n",
    "    Build a Recurrent Neural Network.\n",
    "    '''\n",
    "    word_input = Input(shape=(max_sent_len, ), dtype='int32', name='word_input') # One word at a time\n",
    "    if pretrained_embeddings:\n",
    "        word_embedding = Embedding(vocab_size, dimensionality, \n",
    "                                   input_length=max_sent_len, weights=[embedding_weights])(word_input)\n",
    "    else:\n",
    "        word_embedding = Embedding(vocab_size, dimensionality, \n",
    "                                   input_length=max_sent_len)(word_input)\n",
    "    \n",
    "    if fancy:\n",
    "        word_embedding = Dropout(0.4)(word_embedding)\n",
    "        word_embedding = BatchNormalization(mode=1)(word_embedding)\n",
    "    \n",
    "    if bidirectional:\n",
    "        forward = rnn_type(output_dim=128, return_sequences=True, \n",
    "                           dropout_W=fancy*0.5, go_backwards=False)(word_embedding)\n",
    "        backward = rnn_type(output_dim=128, return_sequences=True, \n",
    "                            dropout_W=fancy*0.5, go_backwards=True)(word_embedding)\n",
    "        rnn = merge([forward, backward], mode='concat')\n",
    "        if deep:\n",
    "            if fancy:\n",
    "                rnn = Dropout(0.4)(rnn)\n",
    "                rnn = BatchNormalization(mode=1)(rnn)\n",
    "            forward = rnn_type(output_dim=128, return_sequences=True, \n",
    "                               dropout_W=fancy*0.5, go_backwards=False)(rnn)\n",
    "            backward = rnn_type(output_dim=128, return_sequences=True, \n",
    "                                dropout_W=fancy*0.5, go_backwards=True)(rnn)\n",
    "            rnn = merge([forward, backward], mode='concat')\n",
    "    else:\n",
    "        rnn = rnn_type(output_dim=128, dropout_W=fancy*0.5, return_sequences=True)(word_embedding)\n",
    "        if deep:\n",
    "            rnn = rnn_type(output_dim=128, dropout_W=fancy*0.5, return_sequences=True)(rnn)\n",
    "    \n",
    "    if fancy:\n",
    "        rnn = Dropout(0.4)(rnn)\n",
    "        rnn = BatchNormalization(mode=1)(rnn)\n",
    "        \n",
    "    if fancy:\n",
    "        hidden = TimeDistributed(Dense(n_classes, activation='relu', name='extra_hidden'))(rnn)\n",
    "    else:\n",
    "        hidden = TimeDistributed(Dense(n_classes, activation='sigmoid', name='extra_hidden'))(rnn)\n",
    "    \n",
    "    model_output = TimeDistributed(Dense(n_classes, activation='softmax', name='main_output'))(hidden)\n",
    "    model = Model(input=[word_input], output=[model_output])\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "8b8a64ae-caeb-4f6b-b1ef-3aac6dcbe9ff"
    }
   },
   "source": [
    "Try running the following block, and make a note of the results.\n",
    "\n",
    "Try changing some parameters again (bidirectional RNN, type of RNN, going deep, using 'fancy' stuff) and see how things change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "837be21c-cdf0-4768-94d3-fced4e38abe2"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "word_input (InputLayer)          (None, 20)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_15 (Embedding)         (None, 20, 64)        849216      word_input[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "simplernn_8 (SimpleRNN)          (None, 20, 128)       24704       embedding_15[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "simplernn_9 (SimpleRNN)          (None, 20, 128)       24704       embedding_15[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "merge_4 (Merge)                  (None, 20, 256)       0           simplernn_8[0][0]                \n",
      "                                                                   simplernn_9[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_15 (TimeDistribut(None, 20, 20)        5140        merge_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_16 (TimeDistribut(None, 20, 20)        420         timedistributed_15[0][0]         \n",
      "====================================================================================================\n",
      "Total params: 904184\n",
      "____________________________________________________________________________________________________\n",
      "Train on 3151 samples, validate on 305 samples\n",
      "Epoch 1/10\n",
      "4s - loss: 1.0552 - actual_accuracy: 0.5847 - val_loss: 0.6494 - val_actual_accuracy: 0.8034\n",
      "Epoch 2/10\n",
      "4s - loss: 0.4135 - actual_accuracy: 0.8697 - val_loss: 0.3492 - val_actual_accuracy: 0.8737\n",
      "Epoch 3/10\n",
      "4s - loss: 0.2519 - actual_accuracy: 0.8999 - val_loss: 0.2652 - val_actual_accuracy: 0.8965\n",
      "Epoch 4/10\n",
      "6s - loss: 0.1891 - actual_accuracy: 0.9205 - val_loss: 0.2399 - val_actual_accuracy: 0.8976\n",
      "Epoch 5/10\n",
      "6s - loss: 0.1539 - actual_accuracy: 0.9362 - val_loss: 0.2227 - val_actual_accuracy: 0.8970\n",
      "Epoch 6/10\n",
      "5s - loss: 0.1291 - actual_accuracy: 0.9446 - val_loss: 0.2227 - val_actual_accuracy: 0.8986\n",
      "Epoch 7/10\n",
      "5s - loss: 0.1096 - actual_accuracy: 0.9536 - val_loss: 0.2128 - val_actual_accuracy: 0.9020\n",
      "Epoch 8/10\n",
      "6s - loss: 0.0941 - actual_accuracy: 0.9604 - val_loss: 0.2258 - val_actual_accuracy: 0.8957\n",
      "Epoch 9/10\n",
      "5s - loss: 0.0813 - actual_accuracy: 0.9662 - val_loss: 0.2193 - val_actual_accuracy: 0.9006\n",
      "Epoch 10/10\n",
      "4s - loss: 0.0686 - actual_accuracy: 0.9714 - val_loss: 0.2306 - val_actual_accuracy: 0.8965\n",
      "Evaluation - test_loss: 0.2311 - test_acc: 0.8852\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"410pt\" viewBox=\"0.00 0.00 364.00 410.00\" width=\"364pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 406)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-406 360,-406 360,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 4834105168 -->\n",
       "<g class=\"node\" id=\"node1\"><title>4834105168</title>\n",
       "<polygon fill=\"none\" points=\"99.5,-365.5 99.5,-401.5 255.5,-401.5 255.5,-365.5 99.5,-365.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"177.5\" y=\"-379.8\">word_input (InputLayer)</text>\n",
       "</g>\n",
       "<!-- 4834105680 -->\n",
       "<g class=\"node\" id=\"node2\"><title>4834105680</title>\n",
       "<polygon fill=\"none\" points=\"89,-292.5 89,-328.5 266,-328.5 266,-292.5 89,-292.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"177.5\" y=\"-306.8\">embedding_15 (Embedding)</text>\n",
       "</g>\n",
       "<!-- 4834105168&#45;&gt;4834105680 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>4834105168-&gt;4834105680</title>\n",
       "<path d=\"M177.5,-365.313C177.5,-357.289 177.5,-347.547 177.5,-338.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"181,-338.529 177.5,-328.529 174,-338.529 181,-338.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4761452496 -->\n",
       "<g class=\"node\" id=\"node3\"><title>4761452496</title>\n",
       "<polygon fill=\"none\" points=\"0,-219.5 0,-255.5 169,-255.5 169,-219.5 0,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"84.5\" y=\"-233.8\">simplernn_8 (SimpleRNN)</text>\n",
       "</g>\n",
       "<!-- 4834105680&#45;&gt;4761452496 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>4834105680-&gt;4761452496</title>\n",
       "<path d=\"M154.987,-292.313C142.993,-283.156 128.066,-271.76 115.04,-261.816\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"116.878,-258.815 106.805,-255.529 112.63,-264.379 116.878,-258.815\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4567123728 -->\n",
       "<g class=\"node\" id=\"node4\"><title>4567123728</title>\n",
       "<polygon fill=\"none\" points=\"187,-219.5 187,-255.5 356,-255.5 356,-219.5 187,-219.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"271.5\" y=\"-233.8\">simplernn_9 (SimpleRNN)</text>\n",
       "</g>\n",
       "<!-- 4834105680&#45;&gt;4567123728 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>4834105680-&gt;4567123728</title>\n",
       "<path d=\"M200.255,-292.313C212.378,-283.156 227.466,-271.76 240.631,-261.816\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"243.085,-264.349 248.955,-255.529 238.866,-258.763 243.085,-264.349\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4761403024 -->\n",
       "<g class=\"node\" id=\"node5\"><title>4761403024</title>\n",
       "<polygon fill=\"none\" points=\"120,-146.5 120,-182.5 235,-182.5 235,-146.5 120,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"177.5\" y=\"-160.8\">merge_4 (Merge)</text>\n",
       "</g>\n",
       "<!-- 4761452496&#45;&gt;4761403024 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>4761452496-&gt;4761403024</title>\n",
       "<path d=\"M107.013,-219.313C119.007,-210.156 133.934,-198.76 146.96,-188.816\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"149.37,-191.379 155.195,-182.529 145.122,-185.815 149.37,-191.379\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4567123728&#45;&gt;4761403024 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>4567123728-&gt;4761403024</title>\n",
       "<path d=\"M248.745,-219.313C236.622,-210.156 221.534,-198.76 208.369,-188.816\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"210.134,-185.763 200.045,-182.529 205.915,-191.349 210.134,-185.763\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4761400464 -->\n",
       "<g class=\"node\" id=\"node6\"><title>4761400464</title>\n",
       "<polygon fill=\"none\" points=\"63.5,-73.5 63.5,-109.5 291.5,-109.5 291.5,-73.5 63.5,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"177.5\" y=\"-87.8\">timedistributed_15 (TimeDistributed)</text>\n",
       "</g>\n",
       "<!-- 4761403024&#45;&gt;4761400464 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>4761403024-&gt;4761400464</title>\n",
       "<path d=\"M177.5,-146.313C177.5,-138.289 177.5,-128.547 177.5,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"181,-119.529 177.5,-109.529 174,-119.529 181,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4806362576 -->\n",
       "<g class=\"node\" id=\"node7\"><title>4806362576</title>\n",
       "<polygon fill=\"none\" points=\"63.5,-0.5 63.5,-36.5 291.5,-36.5 291.5,-0.5 63.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"177.5\" y=\"-14.8\">timedistributed_16 (TimeDistributed)</text>\n",
       "</g>\n",
       "<!-- 4761400464&#45;&gt;4806362576 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>4761400464-&gt;4806362576</title>\n",
       "<path d=\"M177.5,-73.3129C177.5,-65.2895 177.5,-55.5475 177.5,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"181,-46.5288 177.5,-36.5288 174,-46.5289 181,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs              = 10    # Number of epochs\n",
    "bidirectional         = True # Using bidirectional (True/False)\n",
    "pretrained_embeddings = True  # Using pretrained embeddings (True/False)\n",
    "rnn_type              = SimpleRNN  # One out of: SimpleRNN, GRU, LSTM\n",
    "fancy                 = False # Use modern stuff (True/False) (Adds: Dropout, BatchNormalisation, ReLu)\n",
    "deep                  = False # Add another layer of recurrent connections\n",
    "\n",
    "\n",
    "model = build_rnn(bidirectional, pretrained_embeddings, rnn_type, fancy, deep)\n",
    "model.compile(optimizer='adam',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=[actual_accuracy,])\n",
    "model.summary()\n",
    "\n",
    "model.fit(train_X_rnn, train_y_rnn, validation_data=(dev_X_rnn, dev_y_rnn), nb_epoch=n_epochs, batch_size=10, verbose=2, callbacks=[EarlyStopping(monitor='val_loss', patience=2)]);\n",
    "print('Evaluation - test_loss: {0:.4f} - test_acc: {1:.4f}'.format(*model.evaluate(test_X_rnn, test_y_rnn, batch_size=10, verbose=2)))\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b0b1e4ac-f411-4080-a93e-250e6722df16"
    }
   },
   "source": [
    "You should be able to get somewhere around 90 - 92% accuracy on both dev and test.\n",
    "That's an error reduction of about 50% compared to a standard FFNN!\n",
    "\n",
    "Let's do the same error analysis as in the previous case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "f5fbbb43-aa33-4a61-b669-37a890079fcd"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNT: [u'Det', u'k\\xe4nns', u'riktigt', u'att', u'ha', u'ett', u'riktigt', u'yrke', u'.']\n",
      "POS: [u'PRON', u'VERB', u'ADV', u'SCONJ', u'VERB', u'DET', u'ADJ', u'NOUN', u'PUNCT']\n",
      " UNK:[False, False, False, False, False, False, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "def rnn_predict_sentence(sent, model):\n",
    "    word_ids = [word_to_id.get(word, word_to_id['_UNK']) for word in sent]\n",
    "    padded = sequence.pad_sequences([word_ids], maxlen=max_sent_len, dtype=np.int32, value=word_to_id['<PAD>'])\n",
    "    preds = model.predict_on_batch(padded)\n",
    "    pred_pos = [tag for pred in preds[0] for tag, tag_id in tag_to_id.items() if tag_id == np.argmax(pred)]\n",
    "    print('SNT: {0}'.format(sent))\n",
    "    print('POS: {0}\\n UNK:{1}'.format(pred_pos[-len(sent):], [idx==word_to_id['_UNK'] for idx in word_ids]))\n",
    "    \n",
    "\n",
    "sent = u'Det känns riktigt att ha ett riktigt yrke .'.split()\n",
    "rnn_predict_sentence(sent, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "021d56d7-cffd-420f-b592-653c62a2e85b"
    }
   },
   "source": [
    "We now get both types of 'riktigt' right, thanks to the RNN taking advantage of context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "aa9fddc0-4baa-4622-a098-2694b763276b"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nbpresent": {
   "slides": {},
   "themes": {
    "default": "70cd699b-d195-43ef-8ab8-d2905453470b",
    "theme": {}
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
