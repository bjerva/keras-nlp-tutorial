{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP with Keras\n",
    "\n",
    "Begin by trying to import the necessary packages (click the cell, and Shift+Enter):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1337)  # Freeze seeds for reproducibility\n",
    "import keras\n",
    "\n",
    "from codecs import open\n",
    "from collections import defaultdict\n",
    "\n",
    "# Prepare word/tag to ID conversions\n",
    "fillers = ('<w>', '</w>', '<PAD>', '_UNK')\n",
    "word_to_id = defaultdict(lambda: len(word_to_id)+1) # 0 for padding\n",
    "tag_to_id = defaultdict(lambda: len(tag_to_id)+1)\n",
    "for word in fillers:\n",
    "    word_to_id[word]\n",
    "    tag_to_id[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that worked, you're good to go!\n",
    "\n",
    "Next, load these predefined functions for reading and preparing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_ffnn_data(fname, delimiter='\\t'):\n",
    "    '''Read data from a tsv file with format: <word<TAB>tag<NEWLINE>'''\n",
    "    X, y = [], []\n",
    "    with open(fname, 'r', encoding='utf-8') as in_f:\n",
    "        for line in in_f:\n",
    "            if len(line) <= 2: continue\n",
    "            word, tag = line.strip().split(delimiter)\n",
    "            X.append([word_to_id.get(word, word_to_id['_UNK'])])\n",
    "            y.append(tag_to_id[tag])\n",
    "            \n",
    "    return X, y\n",
    "\n",
    "def read_rnn_data(fname, delimiter='\\t'):\n",
    "    '''\n",
    "    Read data from a tsv file with format: <word<TAB>tag<NEWLINE>,\n",
    "    and blank lines between each sentence.\n",
    "    '''\n",
    "    X, y = [], []\n",
    "    with open(fname, 'r', encoding='utf-8') as in_f:\n",
    "        curr_X, curr_y = [word_to_id['<w>']], [tag_to_id['<w>']]\n",
    "        for line in in_f:\n",
    "            if len(line) <= 2: # i.e. newline\n",
    "                if curr_X and curr_y:\n",
    "                    if len(curr_X) <= 20:\n",
    "                        curr_X.append(word_to_id['</w>'])\n",
    "                        curr_y.append(tag_to_id['</w>'])\n",
    "                        X.append(curr_X)\n",
    "                        y.append(curr_y)\n",
    "                    curr_X = [word_to_id['<w>']]\n",
    "                    curr_y = [tag_to_id['<w>']]\n",
    "                continue\n",
    "            word, tag = line.strip().split(delimiter)\n",
    "#             if word in word_to_id:\n",
    "#                 import pdb; pdb.set_trace()\n",
    "            curr_X.append(word_to_id.get(word, word_to_id['_UNK']))\n",
    "            curr_y.append(tag_to_id[tag])\n",
    "            \n",
    "    # Catch last line\n",
    "    if X and y:\n",
    "        curr_X.append(word_to_id['</w>'])\n",
    "        curr_y.append(tag_to_id['</w>'])\n",
    "        X.append(curr_X)\n",
    "        y.append(curr_y)\n",
    "            \n",
    "    return X, y\n",
    "\n",
    "def read_word_embeddings(fname):\n",
    "    '''Read an embeddings file matching the input vocab'''\n",
    "    word_vec_map = {}\n",
    "    with open(fname, 'r', encoding='utf-8') as in_f:\n",
    "        for line in in_f:\n",
    "            fields = line.strip().split()\n",
    "            word = fields[0]\n",
    "            embedding = np.asarray([float(i) for i in fields[1:]], dtype=np.float32)\n",
    "            word_to_id[word]\n",
    "            word_vec_map[word] = embedding\n",
    "    \n",
    "    return word_vec_map, len(embedding)\n",
    "\n",
    "def y_to_onehot(y):\n",
    "    y_onehot = np.zeros((len(y), n_classes), dtype=np.int32)\n",
    "    for idx, tag in enumerate(y):\n",
    "        y_onehot[idx, tag] = 1\n",
    "        \n",
    "    return y_onehot\n",
    "\n",
    "def y_to_onehot_rnn(y):\n",
    "    y_onehot = np.zeros((len(y), max_sent_len, n_classes), dtype=np.int32)\n",
    "   # y_onehot[:,:,tag_to_id['<PAD>']] = 1\n",
    "    for idx, sentence in enumerate(y):\n",
    "        for idy, tag in enumerate(sentence):\n",
    "            if idy >= max_sent_len: break\n",
    "           # y_onehot[idx, idy+(max_sent_len-len(sentence)), tag_to_id['<PAD>']] = 0\n",
    "            y_onehot[idx, idy+(max_sent_len-len(sentence)), tag] = 1\n",
    "            \n",
    "    sequence.pad_sequences(y_onehot, maxlen=max_sent_len, dtype=np.int32, value=tag_to_id['<PAD>'])\n",
    "            \n",
    "    return y_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-Forward Neural Networks\n",
    "\n",
    "So that our neural network can make sense of our data, we'll be working with numerical IDs from here on.\n",
    "\n",
    "We'll also load some pre-trained word representations. Such 'word embeddings' are a big part of the success of neural NLP applications. Using very simple techniques, distributional word representatiosn can be learnt both quickly and without any supervision at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_to_vec, dimensionality = read_word_embeddings('sv.polyglot.filtered')\n",
    "vocab_size = len(word_to_vec)+1\n",
    "embedding_weights = np.zeros((vocab_size, dimensionality), dtype=np.float32)\n",
    "for word, index in word_to_id.items():\n",
    "    embedding_weights[index,:] = word_to_vec[word]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at how some word representations are similar/different to each other (*low = similar, high = dissimilar*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cos(gul, vit): 0.199402826075\n",
      "cos(usa, ger): 0.130351893484\n",
      "cos(vit, ger): 0.832578361366\n",
      "cos(usa, gul): 0.908857627253\n"
     ]
    }
   ],
   "source": [
    "gul = word_to_vec['gul']\n",
    "vit = word_to_vec['vit']\n",
    "usa = word_to_vec['USA']\n",
    "ger = word_to_vec['Tyskland']\n",
    "import scipy\n",
    "print('cos(gul, vit): {0}'.format(scipy.spatial.distance.cosine(gul, vit)))\n",
    "print('cos(usa, ger): {0}'.format(scipy.spatial.distance.cosine(usa, ger)))\n",
    "print('cos(vit, ger): {0}'.format(scipy.spatial.distance.cosine(vit, ger)))\n",
    "print('cos(usa, gul): {0}'.format(scipy.spatial.distance.cosine(usa, gul)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll load some Part-of-Speech (POS) data.\n",
    "We'll use a subset of the Swedish Universal Dependencies POS data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_X, train_y = read_ffnn_data('sv-ud-train-small.conllu')\n",
    "dev_X, dev_y = read_ffnn_data('sv-ud-dev-small.conllu')\n",
    "test_X, test_y = read_ffnn_data('sv-ud-test-small.conllu')\n",
    "\n",
    "vocab_size = len(word_to_vec)+1\n",
    "n_classes = len(tag_to_id)+1\n",
    "\n",
    "train_y = y_to_onehot(train_y)\n",
    "dev_y = y_to_onehot(dev_y)\n",
    "test_y = y_to_onehot(test_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our first experiments, we're using a simple type of Neural Networks: The Feed-Forward Neural Network (FFNN).\n",
    "\n",
    "(TODO: Figure and explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers.core import Dense, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "def build_ffnn(n_layers, pretrained_embeddings):\n",
    "    '''\n",
    "    Build a simple Feed-Forward Neural Network.\n",
    "    '''\n",
    "    word_input = Input(shape=(1, ), dtype='int32', name='word_input') # One word at a time\n",
    "    if pretrained_embeddings:\n",
    "        word_embedding = Embedding(vocab_size, dimensionality, input_length=1, weights=[embedding_weights])(word_input)\n",
    "    else:\n",
    "        word_embedding = Embedding(vocab_size, dimensionality, input_length=1)(word_input)\n",
    "        \n",
    "    word_embedding = Flatten()(word_embedding)\n",
    "    \n",
    "    # Hidden layer(s)\n",
    "    layer_dim = dimensionality * 2\n",
    "    hidden = Dense(layer_dim)(word_embedding)\n",
    "    for n in range(n_layers-1):\n",
    "        hidden = Dense(layer_dim)(hidden)\n",
    "        layer_dim = int(layer_dim/2.0)\n",
    "    \n",
    "    model_output = Dense(n_classes, activation='softmax', name='main_output')(hidden)\n",
    "    model = Model(input=[word_input], output=[model_output])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try running the following block, and make a note of the results.\n",
    "\n",
    "Try changing some parameters (n layers, not using pretrained embeddings, n epochs) and see how things change.\n",
    "**Is going deeper better?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "word_input (InputLayer)          (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)          (None, 1, 64)         849216      word_input[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 64)            0           embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 128)           8320        flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "main_output (Dense)              (None, 21)            2709        dense_2[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 860245\n",
      "____________________________________________________________________________________________________\n",
      "Train on 18633 samples, validate on 1917 samples\n",
      "Epoch 1/10\n",
      "0s - loss: 2.5267 - acc: 0.3381 - val_loss: 2.1554 - val_acc: 0.4512\n",
      "Epoch 2/10\n",
      "0s - loss: 1.8143 - acc: 0.5595 - val_loss: 1.6665 - val_acc: 0.5775\n",
      "Epoch 3/10\n",
      "0s - loss: 1.4252 - acc: 0.6603 - val_loss: 1.3440 - val_acc: 0.6980\n",
      "Epoch 4/10\n",
      "0s - loss: 1.1719 - acc: 0.7407 - val_loss: 1.1268 - val_acc: 0.7355\n",
      "Epoch 5/10\n",
      "0s - loss: 1.0040 - acc: 0.7735 - val_loss: 0.9791 - val_acc: 0.7788\n",
      "Epoch 6/10\n",
      "0s - loss: 0.8891 - acc: 0.8088 - val_loss: 0.8735 - val_acc: 0.8169\n",
      "Epoch 7/10\n",
      "0s - loss: 0.8067 - acc: 0.8221 - val_loss: 0.7953 - val_acc: 0.8200\n",
      "Epoch 8/10\n",
      "0s - loss: 0.7451 - acc: 0.8267 - val_loss: 0.7357 - val_acc: 0.8284\n",
      "Epoch 9/10\n",
      "1s - loss: 0.6973 - acc: 0.8338 - val_loss: 0.6892 - val_acc: 0.8466\n",
      "Epoch 10/10\n",
      "0s - loss: 0.6591 - acc: 0.8366 - val_loss: 0.6512 - val_acc: 0.8524\n",
      "Evaluation - test_loss: 0.7178 - test_acc: 0.8063\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10                # Number of epochs\n",
    "n_layers = 1                 # Number of hidden layers\n",
    "pretrained_embeddings = True # Using pretrained embeddings (True/False)\n",
    "\n",
    "model = build_ffnn(n_layers, pretrained_embeddings)\n",
    "model.compile(optimizer='sgd',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "model.fit(train_X, train_y, validation_data=(dev_X, dev_y), nb_epoch=n_epochs, batch_size=100, verbose=2);\n",
    "print('Evaluation - test_loss: {0:.4f} - test_acc: {1:.4f}'.format(*model.evaluate(test_X, test_y, batch_size=100, verbose=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pre-trained embeddings, you should have gotten a test accuracy of above 80%.\n",
    "Not too bad!\n",
    "\n",
    "Now, let's do a small error analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNT: [u'Det', u'k\\xe4nns', u'riktigt', u'att', u'ha', u'ett', u'riktigt', u'yrke', u'.']\n",
      "POS: [u'PRON', u'VERB', u'ADJ', u'PART', u'VERB', u'DET', u'ADJ', u'NOUN', u'PUNCT']\n",
      " UNK:[False, False, False, False, False, False, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "def ffnn_predict_sentence(sent, model):\n",
    "    word_ids = [word_to_id.get(word, word_to_id['_UNK']) for word in sent]\n",
    "    preds = model.predict_on_batch(word_ids)\n",
    "    pred_pos = [tag for pred in preds for tag, tag_id in tag_to_id.items() if tag_id == np.argmax(pred)]\n",
    "    print('SNT: {0}'.format(sent))\n",
    "    print('POS: {0}\\n UNK:{1}'.format(pred_pos, [idx==word_to_id['_UNK'] for idx in word_ids]))\n",
    "    \n",
    "\n",
    "sent = u'Det känns riktigt att ha ett riktigt yrke .'.split()\n",
    "ffnn_predict_sentence(sent, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since no context is used, the two instances of 'riktigt' incorrectly receive the same class (should be ADV, ADJ)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks\n",
    "\n",
    "Using an FFNN is often sufficient, but incorporating context information (of varying lengths) is problematic.\n",
    "\n",
    "A solution to this, is to use Recurrent Neural Networks.\n",
    "(TODO: More info)\n",
    "\n",
    "First, we'll need to read in the data and convert it to a different format.\n",
    "Whereas we used a simple 'one word, one label' format for the FFNN, we now need a format corresponding to a sentence with tags.\n",
    "\n",
    "We also need to specify a maximum sentence length to consider - to keep things running on CPUs in a reasonable time, we'll restrict ourselves to sentences of 20 words or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "def actual_accuracy(act, pred):\n",
    "    '''\n",
    "    Calculate accuracy each batch.\n",
    "    Keras' standard calculation factors in our padding classes. We don't.\n",
    "    '''\n",
    "    act_argm  = K.argmax(act, axis=-1)   # Indices of act. classes\n",
    "    pred_argm = K.argmax(pred, axis=-1)  # Indices of pred. classes\n",
    "\n",
    "    incorrect = K.cast(K.not_equal(act_argm, pred_argm), dtype='float32')\n",
    "    correct   = K.cast(K.equal(act_argm, pred_argm), dtype='float32')\n",
    "    padding   = K.cast(K.equal(K.sum(act), 0), dtype='float32')\n",
    "    start     = K.cast(K.equal(act_argm, 0), dtype='float32')\n",
    "    end       = K.cast(K.equal(act_argm, 1), dtype='float32')\n",
    "\n",
    "    pad_start     = K.maximum(padding, start)\n",
    "    pad_start_end = K.maximum(pad_start, end) # 1 where pad, start or end\n",
    "\n",
    "    # Subtract pad_start_end from correct, then check equality to 1\n",
    "    # E.g.: act: [pad, pad, pad, <s>, tag, tag, tag, </s>]\n",
    "    #      pred: [pad, tag, pad, <s>, tag, tag, err, </s>]\n",
    "    #   correct: [1,     0,   1,   1,   1,   1,   0,    1]\n",
    "    #     p_s_e: [1,     1,   1,   1,,  0,   0,   0,    1]\n",
    "    #  corr-pse: [0,    -1,   0,   0,   1,   1,   0,    0] # Subtraction\n",
    "    # actu_corr: [0,     0,   0,   0,   1,   1,   0,    0] # Check equality to 1\n",
    "    corr_preds   = K.sum(K.cast(K.equal(correct - pad_start_end, 1), dtype='float32'))\n",
    "    incorr_preds = K.sum(K.cast(K.equal(incorrect - pad_start_end, 1), dtype='float32'))\n",
    "    total = corr_preds + incorr_preds\n",
    "    accuracy = corr_preds / total\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "train_X, train_y = read_rnn_data('sv-ud-train.conllu')\n",
    "dev_X, dev_y = read_rnn_data('sv-ud-dev.conllu')\n",
    "test_X, test_y = read_rnn_data('sv-ud-test-small.conllu')\n",
    "\n",
    "max_sent_len = 20#max(len(sent) for sent in train_X)\n",
    "vocab_size = len(word_to_vec)+1\n",
    "n_classes = len(tag_to_id)+1\n",
    "\n",
    "train_y = y_to_onehot_rnn(train_y)\n",
    "dev_y = y_to_onehot_rnn(dev_y)\n",
    "test_y = y_to_onehot_rnn(test_y)\n",
    "\n",
    "train_X = sequence.pad_sequences(train_X, maxlen=max_sent_len, dtype=np.int32, value=word_to_id['<PAD>'])\n",
    "dev_X = sequence.pad_sequences(dev_X, maxlen=max_sent_len, dtype=np.int32, value=word_to_id['<PAD>'])\n",
    "test_X = sequence.pad_sequences(test_X, maxlen=max_sent_len, dtype=np.int32, value=word_to_id['<PAD>'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32),\n",
       " array([    3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "            3,     3,     3,     3,     3,     1, 10668,  7379,     9,\n",
       "            4,     2], dtype=int32))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[0],train_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, SimpleRNN, merge, BatchNormalization\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "def build_rnn(bidirectional, pretrained_embeddings, rnn_type, fancy, deep):\n",
    "    '''\n",
    "    Build a Recurrent Neural Network.\n",
    "    '''\n",
    "    word_input = Input(shape=(max_sent_len, ), dtype='int32', name='word_input') # One word at a time\n",
    "    if pretrained_embeddings:\n",
    "        word_embedding = Embedding(vocab_size, dimensionality, input_length=max_sent_len, weights=[embedding_weights])(word_input)\n",
    "    else:\n",
    "        word_embedding = Embedding(vocab_size, dimensionality, input_length=max_sent_len)(word_input)\n",
    "    \n",
    "    if fancy:\n",
    "        word_embedding = Dropout(0.4)(word_embedding)\n",
    "        word_embedding = BatchNormalization(mode=1)(word_embedding)\n",
    "    \n",
    "    if bidirectional:\n",
    "        forward = rnn_type(output_dim=128, return_sequences=True, dropout_W=fancy*0.5, go_backwards=False)(word_embedding)\n",
    "        backward = rnn_type(output_dim=128, return_sequences=True, dropout_W=fancy*0.5, go_backwards=True)(word_embedding)\n",
    "        rnn = merge([forward, backward], mode='concat')\n",
    "        if deep:\n",
    "            if fancy:\n",
    "                rnn = Dropout(0.4)(rnn)\n",
    "                rnn = BatchNormalization(mode=1)(rnn)\n",
    "            forward = rnn_type(output_dim=128, return_sequences=True, dropout_W=fancy*0.5, go_backwards=False)(rnn)\n",
    "            backward = rnn_type(output_dim=128, return_sequences=True, dropout_W=fancy*0.5, go_backwards=True)(rnn)\n",
    "            rnn = merge([forward, backward], mode='concat')\n",
    "    else:\n",
    "        rnn = rnn_type(output_dim=128, dropout_W=fancy*0.5, return_sequences=True)(word_embedding)\n",
    "        if deep:\n",
    "            rnn = rnn_type(output_dim=128, dropout_W=fancy*0.5, return_sequences=True)(rnn)\n",
    "    \n",
    "    if fancy:\n",
    "        rnn = Dropout(0.4)(rnn)\n",
    "        rnn = BatchNormalization(mode=1)(rnn)\n",
    "        \n",
    "    if fancy:\n",
    "        hidden = TimeDistributed(Dense(n_classes, activation='relu', name='extra_hidden'))(rnn)\n",
    "    else:\n",
    "        hidden = TimeDistributed(Dense(n_classes, activation='sigmoid', name='extra_hidden'))(rnn)\n",
    "    \n",
    "    model_output = TimeDistributed(Dense(n_classes, activation='softmax', name='main_output'))(hidden)\n",
    "    model = Model(input=[word_input], output=[model_output])\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "word_input (InputLayer)          (None, 20)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)          (None, 20, 64)        849216      word_input[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "simplernn_1 (SimpleRNN)          (None, 20, 128)       24704       embedding_3[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_1 (TimeDistribute(None, 20, 21)        2709        simplernn_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_2 (TimeDistribute(None, 20, 21)        462         timedistributed_1[0][0]          \n",
      "====================================================================================================\n",
      "Total params: 877091\n",
      "____________________________________________________________________________________________________\n",
      "Train on 3151 samples, validate on 305 samples\n",
      "Epoch 1/10\n",
      "3s - loss: 1.0431 - actual_accuracy: 0.6173 - val_loss: 0.5983 - val_actual_accuracy: 0.8196\n",
      "Epoch 2/10\n",
      "3s - loss: 0.3928 - actual_accuracy: 0.8766 - val_loss: 0.3401 - val_actual_accuracy: 0.8797\n",
      "Epoch 3/10\n",
      "3s - loss: 0.2495 - actual_accuracy: 0.9070 - val_loss: 0.2699 - val_actual_accuracy: 0.8945\n",
      "Epoch 4/10\n",
      "3s - loss: 0.1924 - actual_accuracy: 0.9233 - val_loss: 0.2409 - val_actual_accuracy: 0.8973\n",
      "Epoch 5/10\n",
      "3s - loss: 0.1579 - actual_accuracy: 0.9363 - val_loss: 0.2238 - val_actual_accuracy: 0.9039\n",
      "Epoch 6/10\n",
      "3s - loss: 0.1342 - actual_accuracy: 0.9446 - val_loss: 0.2162 - val_actual_accuracy: 0.9014\n",
      "Epoch 7/10\n",
      "3s - loss: 0.1157 - actual_accuracy: 0.9519 - val_loss: 0.2164 - val_actual_accuracy: 0.9008\n",
      "Epoch 8/10\n",
      "3s - loss: 0.1007 - actual_accuracy: 0.9575 - val_loss: 0.2130 - val_actual_accuracy: 0.9001\n",
      "Epoch 9/10\n",
      "3s - loss: 0.0871 - actual_accuracy: 0.9637 - val_loss: 0.2149 - val_actual_accuracy: 0.9051\n",
      "Epoch 10/10\n",
      "4s - loss: 0.0771 - actual_accuracy: 0.9674 - val_loss: 0.2248 - val_actual_accuracy: 0.8991\n",
      "Evaluation - test_loss: 0.2397 - test_acc: 0.8958\n"
     ]
    }
   ],
   "source": [
    "n_epochs              = 10    # Number of epochs\n",
    "bidirectional         = False # Using bidirectional (True/False)\n",
    "pretrained_embeddings = True  # Using pretrained embeddings (True/False)\n",
    "rnn_type              = SimpleRNN  # One out of: SimpleRNN, GRU, LSTM\n",
    "fancy                 = False # Use modern stuff (True/False) (Adds: Dropout, BatchNormalisation, ReLu)\n",
    "deep                  = False # Add another layer of recurrent connections\n",
    "\n",
    "\n",
    "model = build_rnn(bidirectional, pretrained_embeddings, rnn_type, fancy, deep)\n",
    "model.compile(optimizer='adam',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=[actual_accuracy,])\n",
    "model.summary()\n",
    "\n",
    "model.fit(train_X, train_y, validation_data=(dev_X, dev_y), nb_epoch=n_epochs, batch_size=10, verbose=2, callbacks=[EarlyStopping(monitor='val_loss', patience=2)]);\n",
    "print('Evaluation - test_loss: {0:.4f} - test_acc: {1:.4f}'.format(*model.evaluate(test_X, test_y, batch_size=10, verbose=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to get somewhere around 90 - 92% accuracy on both dev and test.\n",
    "That's an error reduction of about 50% compared to a standard FFNN!\n",
    "\n",
    "Let's do the same error analysis as in the previous case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNT: [u'Det', u'k\\xe4nns', u'riktigt', u'att', u'ha', u'ett', u'riktigt', u'yrke', u'.']\n",
      "POS: [u'PRON', u'VERB', u'ADV', u'PART', u'VERB', u'DET', u'ADJ', u'NOUN', u'PUNCT']\n",
      " UNK:[False, False, False, False, False, False, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "def rnn_predict_sentence(sent, model):\n",
    "    word_ids = [word_to_id.get(word, word_to_id['_UNK']) for word in sent]\n",
    "    padded = sequence.pad_sequences([word_ids], maxlen=max_sent_len, dtype=np.int32, value=word_to_id['<PAD>'])\n",
    "    preds = model.predict_on_batch(padded)\n",
    "    pred_pos = [tag for pred in preds[0] for tag, tag_id in tag_to_id.items() if tag_id == np.argmax(pred)]\n",
    "    print('SNT: {0}'.format(sent))\n",
    "    print('POS: {0}\\n UNK:{1}'.format(pred_pos[-len(sent):], [idx==word_to_id['_UNK'] for idx in word_ids]))\n",
    "    \n",
    "\n",
    "sent = u'Det känns riktigt att ha ett riktigt yrke .'.split()\n",
    "rnn_predict_sentence(sent, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now get both types of 'riktigt' right, thanks to the RNN taking advantage of context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
